# Kleros Juror Profile Generator - Development & Testing Runbook

**(Brief Description - same as before)**

[![License: MIT](...)]()
[![Python Version](...)]()

## Overview

**(Same as previous version, including Agentic AI mention and Ethical Considerations)**

## Table of Contents

* [Who is this for?](#who-is-this-for)
* [Prerequisites](#prerequisites)
* [Getting Started](#getting-started)
    * [Cloning the Repository](#cloning-the-repository)
    * [Installation & Setup](#installation--setup)
    * [Backend Configuration (LLM Access)](#backend-configuration-llm-access)
* [Runbook Sections](#runbook-sections)
    * [1. Development Environment](./docs/development.md)
    * [2. Customizing Juror Generation](./docs/customization.md)
    * [3. Generating Test Profiles (Running the Tool)](./docs/generating-profiles.md)
* [Technology Stack](#technology-stack)
* [Output Format & Traceability](#output-format--traceability)
* [Further Reading / Conceptual Background](#further-reading--conceptual-background)
* [Troubleshooting](#troubleshooting)
* [Contributing](#contributing) (Optional)
* [License](#license)
* [Contact](#contact) (Optional)

## Who is this for?

**(Same as previous version)**

## Prerequisites

**(Same as previous version, ensure it lists Python, pip, Git, LLM access/keys)**

## Getting Started

**(Cloning - same as previous version)**

### Installation & Setup

**(Same as previous version - Virtual env, pip install)**

### Backend Configuration (LLM Access)

This tool is designed to be potentially adaptable to various LLM backends. The core configuration happens via environment variables and potentially configuration files (`config.yaml`).

1.  **Environment Variables (`.env`):** Primarily used for API keys and secrets.
    ```bash
    cp .env.example .env
    # Edit .env with your specific keys and endpoints
    ```
    * **Example Variables:**
        ```dotenv
        # For OpenAI
        OPENAI_API_KEY="sk-..."
        # OPENAI_MODEL_NAME="gpt-4-turbo-preview"

        # For Azure OpenAI
        # AZURE_OPENAI_ENDPOINT="[https://your-instance.openai.azure.com/](https://www.google.com/search?q=https://your-instance.openai.azure.com/)"
        # AZURE_OPENAI_API_KEY="your-azure-key"
        # AZURE_DEPLOYMENT_NAME="your-deployment-id" # Model deployment name

        # For Google Cloud Vertex AI
        # GOOGLE_APPLICATION_CREDENTIALS="/path/to/your/keyfile.json"
        # GCP_PROJECT_ID="your-gcp-project-id"
        # GCP_LOCATION="us-central1"
        # VERTEX_MODEL_NAME="gemini-1.0-pro" # Or other model ID

        # For Hugging Face (Inference Endpoints or local)
        # HF_API_TOKEN="hf_..." # For gated models or Inference API
        # HF_MODEL_NAME="mistralai/Mistral-7B-Instruct-v0.1" # Example

        # For Local/Open Source (if using something like Ollama or LM Studio)
        # LOCAL_LLM_API_BASE="http://localhost:11434/v1" # Example for Ollama-compatible API
        # LOCAL_LLM_MODEL_NAME="llama3"
        # LOCAL_LLM_API_KEY="ollama" # Or often not needed
        ```
    * **Important:** Ensure `.env` is in `.gitignore`.

2.  **Configuration File (`config.yaml` - Optional):** For non-sensitive settings like default model parameters, prompt template paths, etc.
    ```yaml
    # Example config.yaml structure
    # llm_defaults:
    #   temperature: 0.7
    #   max_tokens: 500
    # prompt_paths:
    #   juror_base_prompt: "./prompts/juror_base_v1.txt"
    # backend_settings:
    #   # Platform-specific overrides if not using env vars alone
    #   # e.g., azure_api_version: "2023-07-01-preview"
    ```

**(The actual code needs logic to read these variables/configs and instantiate the correct LLM client.)**

## Runbook Sections

1.  **[Development Environment](./docs/development.md):** Setting up locally.
2.  **[Customizing Juror Generation](./docs/customization.md):** Modifying the logic. Key areas include:
    * **Prompt Engineering:** Adjusting the text prompts sent to the LLM (often the biggest impact). Files might be in a `/prompts` directory.
    * **Generation Parameters:** Tweaking settings like `temperature`, `top_p`, `max_tokens` (often configurable via `config.yaml` or command-line args).
    * **Backend Logic:** Modifying the Python code (`generate_jurors.py` or similar) to handle different LLM clients (OpenAI, Anthropic, VertexAI, Hugging Face Transformers, local models via libraries like `litellm` or custom wrappers), potentially using different parameters or prompt formats suited to each.
    * **Archetype Definition:** Changing how different types of jurors are defined or requested.
3.  **[Generating Test Profiles (Running the Tool)](./docs/generating-profiles.md):** Executing the main generation script (e.g., `python generate_jurors.py --count 10 --output profiles.json`). This guide details command-line arguments and usage.

## Technology Stack

* **Core Language:** Python [Version]
* **LLM Interaction:** [e.g., OpenAI Lib, Langchain, Hugging Face Transformers, Google Vertex AI SDK, LiteLLM (for multi-backend support)]
* **Supported Backends (Potentially):** OpenAI API, Azure OpenAI, Google Vertex AI, Anthropic API, Hugging Face (Inference API/Transformers), Local models (via Ollama, LM Studio compatible APIs). *(Adjust based on actual implementation)*
* **Agent Concepts/Patterns:** *(Same as before)*
* **Data Handling:** *(Same as before)*
* **Environment Management:** python-dotenv, potentially YAML reader (PyYAML).
* **[Other Libraries]:** *(Same as before)*

## Output Format & Traceability

The tool outputs profiles (e.g., as JSON Lines - one JSON object per line in a file). A key goal is **traceability** for reproducibility and analysis. Each generated profile should include metadata:

```json
// Example JSON structure for one profile
{
  "jurorId": "sim-juror-abc123",
  "profile": {
    // --- Core generated profile attributes ---
    "background": "Data scientist with a background in behavioral economics.",
    "expertiseAreas": ["statistics", "machine learning", "market analysis"],
    "cognitiveProfile": "Prone to analytical thinking, slightly risk-averse, values empirical evidence.",
    "platformActivity": "Simulated: 25 cases judged, 92% coherence."
    // ... other relevant generated fields
  },
  "generationMetadata": {
    // --- Fields for tracing randomness and context ---
    "generationTimestamp": "2025-04-01T09:30:00Z",
    "llmBackendUsed": "Azure OpenAI", // e.g., "OpenAI", "VertexAI", "HuggingFace", "Local/Ollama"
    "llmModelName": "gpt-4-deployment-xyz", // Specific model or deployment ID used
    "promptTemplateId": "juror_base_v1.2", // Identifier for the prompt structure used
    "generationParameters": { // Key parameters influencing randomness
      "temperature": 0.75,
      "top_p": 1.0,
      "max_tokens": 500
      // "seed": 42 // If the backend/model supports deterministic seeding
    },
    "requestInput": { // Optional: Key inputs used for this specific generation
      "archetypeRequested": "analytical_expert",
      "caseContextSnippet": "Dispute involving smart contract failure..." // If applicable
    }
    // Potentially add version of the generation script itself
    // "scriptVersion": "v0.2.1"
  }
}